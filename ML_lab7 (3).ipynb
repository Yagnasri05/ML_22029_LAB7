{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow tensorflow-io matplotlib xgboost catboost"
      ],
      "metadata": {
        "id": "MCrOd5wyPZ2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD85ntiNJtki",
        "outputId": "e4a80347-1de4-4e5a-a0a2-7a236230787f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#necessary libraries\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "import numpy as np\n",
        "import pywt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "\n",
        "#function to load audio file\n",
        "def load_wav_16k_mono(filename):\n",
        "    # Load encoded wav file\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    # Decode wav (tensors by channels)\n",
        "    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
        "    # Removes trailing axis\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    # Goes from 44100Hz to 16000hz - amplitude of the audio signal\n",
        "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
        "    return wav\n",
        "\n",
        "#listing gunshot n non gunshot files\n",
        "POS = '/content/drive/MyDrive/aidataset/gunshot'\n",
        "NEG = '/content/drive/MyDrive/aidataset/nongunshot'\n",
        "pos = tf.data.Dataset.list_files(POS+'/*.wav')\n",
        "neg = tf.data.Dataset.list_files(NEG+'/*.wav')\n",
        "\n",
        "#labeling positive as 1 and negative as 0\n",
        "positives = tf.data.Dataset.zip((pos, tf.data.Dataset.from_tensor_slices(tf.ones(len(pos)))))\n",
        "negatives = tf.data.Dataset.zip((neg, tf.data.Dataset.from_tensor_slices(tf.zeros(len(neg)))))\n",
        "\n",
        "#calculating the quotient n remainder\n",
        "repeat_count = len(negatives) // len(positives)\n",
        "remainder = len(negatives) % len(positives)\n",
        "#oversampling the minority\n",
        "new_positives = positives.repeat(repeat_count) #repeat that many times as quotient\n",
        "new_positives = new_positives.concatenate(positives.take(remainder)) #add that many as remainder\n",
        "data = new_positives.concatenate(negatives) #concatenate gunshot n nonshot\n",
        "\n",
        "# function provided in tenserflow\n",
        "def preprocess(file_path, label):\n",
        "    wav = load_wav_16k_mono(file_path)\n",
        "    wav = wav[:40000]\n",
        "    #padding to make equal wav\n",
        "    zero_padding = tf.zeros([40000] - tf.shape(wav), dtype=tf.float32)\n",
        "    wav = tf.concat([zero_padding, wav], 0)\n",
        "    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32) #short term fourier transform\n",
        "    spectrogram = tf.abs(spectrogram) #taking absolute of it\n",
        "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
        "    return spectrogram, label\n",
        "\n",
        "#preprocess, shuffling n batch for data\n",
        "data = data.map(preprocess)\n",
        "data = data.cache()\n",
        "data = data.shuffle(buffer_size=5000) #shuffling of data\n",
        "data = data.batch(16)\n",
        "data = data.prefetch(8)\n",
        "\n",
        "#spliting dataset to train,validation n test\n",
        "train = data.take(130)\n",
        "test = data.skip(130).take(30)\n",
        "ttest = data.skip(160).take(20)\n",
        "\n",
        "#CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(1241, 257, 1)))\n",
        "model.add(Conv2D(32, (3,3), activation='relu'))\n",
        "model.add(Dropout(0.5))  # Add dropout layer with dropout rate of 0.5\n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.5))  #Add dropout layer with dropout rate of 0.5\n",
        "\n",
        "# Create a feature extraction model\n",
        "feature_extractor = Model(inputs=model.input, outputs=model.layers[-1].output)\n",
        "\n",
        "# Extract features for training dataset\n",
        "features = []\n",
        "labels = []\n",
        "for spectrogram, label in train:\n",
        "    extracted_features = feature_extractor.predict(spectrogram)\n",
        "    features.append(extracted_features)\n",
        "    labels.append(label)\n",
        "\n",
        "features = np.concatenate(features, axis=0)\n",
        "labels = np.concatenate(labels, axis=0)\n",
        "\n",
        "# Extract features from test dataset\n",
        "test_features = []\n",
        "test_labels = []\n",
        "for spectrogram, label in test:\n",
        "    extracted_features = feature_extractor.predict(spectrogram)\n",
        "    test_features.append(extracted_features)\n",
        "    test_labels.append(label)\n",
        "\n",
        "test_features = np.concatenate(test_features, axis=0)\n",
        "test_labels = np.concatenate(test_labels, axis=0)\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"KNN (k=2)\": KNeighborsClassifier(n_neighbors=2),\n",
        "    \"KNN (k=3)\": KNeighborsClassifier(n_neighbors=3),\n",
        "    \"SVM\": SVC(),\n",
        "    \"Random Forests\": RandomForestClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"CatBoost\": CatBoostClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(),\n",
        "}\n",
        "#MLP Classifier\n",
        "mlp_classifier = MLPClassifier(hidden_layer_sizes=(64, 32),  #structure of the MLP\n",
        "                                       activation='relu',  # Activation function for hidden layers\n",
        "                                       solver='adam',  # Optimization algorithm\n",
        "                                       batch_size=16,\n",
        "                                       random_state=42)  # Random state for reproducibility\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {'max_iter': range(1, 21)}\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Perform Randomized Search CV\n",
        "random_search = RandomizedSearchCV(mlp_classifier, param_distributions=param_grid, n_iter=10)\n",
        "random_search.fit(features, labels)\n",
        "\n",
        "# Get the best parameters\n",
        "print(\"Best parameters:\", random_search.best_params_)\n",
        " #Repeat for Various Values of max iteration\n",
        "accuracies_project = []\n",
        "for k in range(1, 20):\n",
        "    mlp_classifier = MLPClassifier(hidden_layer_sizes=(64, 32),  #structure of the MLP\n",
        "                                       activation='relu',  # Activation function for hidden layers\n",
        "                                       solver='adam',  # Optimization algorithm\n",
        "                                       batch_size=16,\n",
        "                                      max_iter = k ,\n",
        "                                       random_state=42)  # Random state for reproducibility\n",
        "\n",
        "    mlp_classifier.fit(features, labels)\n",
        "    accuracy_project = mlp_classifier.score(test_features, test_labels)\n",
        "    accuracies_project.append(accuracy_project)\n",
        "\n",
        "plt.plot(range(1, 20), accuracies_project)\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs k for max_iter in MLP Classifier')\n",
        "plt.show()\n",
        "\n",
        "# Compute evaluation metrics for Voting Classifier\n",
        "accuracy_results['Voting Classifier'] = accuracy_score(test_labels, y_pred_voting)\n",
        "precision_results['Voting Classifier'] = precision_score(test_labels, y_pred_voting)\n",
        "recall_results['Voting Classifier'] = recall_score(test_labels, y_pred_voting)\n",
        "f1_results['Voting Classifier'] = f1_score(test_labels, y_pred_voting)\n",
        "confusion_matrices['Voting Classifier'] = confusion_matrix(test_labels, y_pred_voting)\n",
        "train_accuracy_results['Voting Classifier'] = accuracy_score(labels, y_train_pred_voting)\n",
        "\n",
        "# Print results\n",
        "for name, result in accuracy_results.items():\n",
        "    print(f\"Classifier: {name}\")\n",
        "    print(f\"Test Accuracy: {result}\")\n",
        "    print(f\"Train Accuracy: {train_accuracy_results[name]}\")\n",
        "    print(f\"Precision: {precision_results[name]}\")\n",
        "    print(f\"Recall: {recall_results[name]}\")\n",
        "    print(f\"F1 Score: {f1_results[name]}\")\n",
        "    print(f\"Confusion Matrix:\\n{confusion_matrices[name]}\\n\")"
      ],
      "metadata": {
        "id": "yVJBh_w0PfF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uve_GXpfcoyA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}